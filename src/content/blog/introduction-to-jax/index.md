---
title: Hello JAXï¼
publishDate: 2025-11-30
heroImage:
  src: ./cat.jpg
  alt: cat
  inferSize: true
description: Introduction to JAXï¼Œfor JAX Code Reading. 
categories:
- Technical
tags:
- python
---

> è¡¥è®°ï¼šå…¶å®è¿™ä¸ªæ–‡ç« çš„è´¨é‡ä¸å¤ªè¡Œï¼Œæˆ‘åªæ˜¯æŠŠJAXåˆ«äººçš„æ•™ç¨‹ç»™ç¿»è¯‘äº†ä¸€éï¼Œæ•´ä½“æ²¡æœ‰è¾“å‡ºè‡ªå·±çš„è§‚ç‚¹ç†è§£ï¼Œè‡ªå·±åæ¥ä¹Ÿè¿˜æ˜¯åŸºäºPytorchåœ¨å¼€å‘æ²¡æœ‰ç”¨JAX,å› ä¸ºæ”¯æŒJAXçš„å¼€æºé¡¹ç›®è¿˜æ˜¯å°‘äº†ã€‚ä»0å¼€å§‹åˆéš¾ä»¥ä»˜å‡ºè¿™æ ·çš„ä»£ä»·ã€‚åç»­æˆ‘ä¼šæ•´ç†è‡ªå·±å¸¸ç”¨çš„æç¤ºè¯ å’Œ ä¸€äº›æœ‰å…³æç¤ºè¯Engineeringçš„å­¦ä¹ ã€‚


|**è§’è‰²**|**åº“/å·¥å…·**|**èŒè´£**|
| --| --| -------------------------------------------------------------------------------------------|
|**å¼•æ“/è®¡ç®—**|**JAX**|è´Ÿè´£åº•å±‚çš„çŸ©é˜µè¿ç®—ã€æ±‚å¯¼ã€ç¼–è¯‘ (æ›¿ä»£`torch.Tensor`â€‹+`autograd`)|
|**ç¥ç»ç½‘ç»œ**|**Flax**|è´Ÿè´£å®šä¹‰å±‚ã€æ¨¡å‹ç»“æ„ (æ›¿ä»£`torch.nn`)|
|**ä¼˜åŒ–å™¨**|**Optax**|è´Ÿè´£ Adam, SGD ç­‰ä¼˜åŒ–ç®—æ³• (æ›¿ä»£`torch.optim`)|
|**æ•°æ®åŠ è½½**|**PyTorch**|**æ²¡é”™ï¼** å³ä½¿æ˜¯ JAX æ•™ç¨‹ï¼Œä¹Ÿå»ºè®®ç»§ç»­ç”¨`torch.utils.data.DataLoader`ï¼Œå› ä¸ºè¿™å— PyTorch åšå¾—æœ€å¥½ï¼Œè€Œä¸”æ•°æ®åŠ è½½ä¸éœ€è¦ GPU åŠ é€Ÿã€‚|

â€‹<kbd>ä¸ºä»€ä¹ˆæˆ‘è¦è‡ªæ‰¾éº»çƒ¦å­¦ JAXï¼Ÿ</kbd>

1. å¿«
2. çœ‹æ‡‚ JAX ç›¸å…³ä»£ç 


**PyTorch (åŠ¨æ€å›¾)** ï¼šé‡‡ç”¨**å³æ—¶æ‰§è¡Œï¼ˆEager Executionï¼‰** æ¨¡å¼ã€‚ç³»ç»Ÿé€è¡Œè§£æä»£ç å¹¶å•ç‹¬è°ƒåº¦æ¯ä¸ªç®—å­åˆ° GPUï¼Œç”±äºç¼ºä¹å¯¹åç»­æ“ä½œçš„å…¨å±€è§†é‡ï¼Œæ— æ³•è¿›è¡Œè·¨ç®—å­çš„ç»Ÿç­¹ä¼˜åŒ–ï¼Œå¯¼è‡´åŸæœ¬å¯å¹¶è¡Œçš„è®¡ç®—å¾€å¾€è¢«ä¸²è¡Œæ‰§è¡Œï¼Œå¢åŠ äº†æ˜¾å­˜è®¿é—®å’Œå†…æ ¸å¯åŠ¨çš„å¼€é”€ã€‚

**JAX (JIT ç¼–è¯‘)** ï¼šé‡‡ç”¨**å³æ—¶ç¼–è¯‘ï¼ˆJIT Compilationï¼‰** æŠ€æœ¯ã€‚ç³»ç»Ÿé€šè¿‡è¿½è¸ªï¼ˆTracingï¼‰æ„å»ºå®Œæ•´çš„é™æ€è®¡ç®—å›¾ï¼Œåˆ©ç”¨ XLA ç¼–è¯‘å™¨è¿›è¡Œ**ç®—å­èåˆï¼ˆOperator Fusionï¼‰** å’ŒæŒ‡ä»¤é‡æ’ã€‚è¿™ä½¿å¾—ç³»ç»Ÿèƒ½æå‰è§„åˆ’æœ€ä¼˜æ‰§è¡Œè·¯å¾„ï¼Œå‡å°‘æ˜¾å­˜è¯»å†™æ¬¡æ•°å¹¶å……åˆ†åˆ©ç”¨ç¡¬ä»¶çš„å¹¶è¡Œè®¡ç®—èƒ½åŠ›ã€‚

ä¸ºäº†è·å¾—è¿™ä¸ªé€Ÿåº¦ï¼Œä½ å¿…é¡»éµå®ˆ **JAX çš„â€œæ¸…è§„æˆ’å¾‹â€** ã€‚è¿™ä¹Ÿæ˜¯ä¸ºä»€ä¹ˆ JAX éš¾ä¸Šæ‰‹çš„åŸå› ã€‚

- **é™åˆ¶ä¸€ï¼šNo Side-Effects**

  - â€‹**PyTorch/Python ä¹ æƒ¯**ï¼šä½ å¯ä»¥åœ¨å‡½æ•°é‡Œæ‚„æ‚„ä¿®æ”¹å…¨å±€å˜é‡ï¼Œæˆ–è€…ä¿®æ”¹è¾“å…¥çš„åˆ—è¡¨ã€‚
  - â€‹**JAX è§„çŸ©**â€‹ï¼šå‡½æ•°åªèƒ½é€šè¿‡ **return** è¿”å›ç»“æœï¼Œä¸èƒ½æ‚„æ‚„æ”¹ä¸œè¥¿ã€‚
  - â€‹**å…¸å‹ç¦å¿Œ**â€‹ï¼šâ€‹**In-place æ“ä½œ**ã€‚

    - âŒ `x[0] = 1` ï¼ˆåŸåœ°ä¿®æ”¹ï¼ŒJAX ç¦æ­¢ï¼‰
    - âœ… `y = x.at[0].set(1)` ï¼ˆåˆ›å»ºä¸€ä¸ªæ–°çš„ xï¼ŒæŠŠå€¼æ”¹äº†èµ‹ç»™ yï¼‰
- **é™åˆ¶äºŒï¼šStatic Shapes**

  - â€‹**è§£é‡Š**ï¼šç¼–è¯‘å™¨ï¼ˆJITï¼‰åœ¨ç¼–è¯‘æ—¶éœ€è¦çŸ¥é“å¼ é‡çš„å…·ä½“å½¢çŠ¶ã€‚
  - â€‹**å…¸å‹ç¦å¿Œ**â€‹ï¼š`y = x[x > 3]`ã€‚

    - å› ä¸º `x`â€‹ é‡Œæœ‰å¤šå°‘ä¸ªå¤§äº 3 çš„æ•°æ˜¯ä¸ç¡®å®šçš„ï¼Œè¿™ä¼šå¯¼è‡´ `y` çš„é•¿åº¦å¿½é•¿å¿½çŸ­ï¼Œç¼–è¯‘å™¨ä¼šç–¯æ‰ã€‚

## JAX as NumPy on accelerators

### DeviceArray ä¸å¼‚æ­¥è°ƒåº¦ (Asynchronous Dispatch)

**æ•°æ®ç»“æ„ï¼šDeviceArray**

- â€‹**å®šä¹‰**â€‹ï¼šJAX çš„æ ¸å¿ƒæ•°æ®ç»“æ„æ˜¯ `jaxlib.xla_extension.DeviceArray`â€‹ï¼ˆåœ¨è¾ƒæ–°ç‰ˆæœ¬ä¸­ç»Ÿä¸€ä¸º `jax.Array`ï¼‰ã€‚
- **å†…å­˜é©»ç•™**ï¼šä¸ NumPy çš„ `ndarray`â€‹ é»˜è®¤é©»ç•™å†…å­˜ä¸åŒï¼ŒJAX çš„ Array é»˜è®¤ç›´æ¥åˆ†é…åœ¨**åŠ é€Ÿå™¨æ˜¾å­˜**ï¼ˆGPU/TPUï¼‰ä¸­ã€‚

â€‹**äº’æ“ä½œæ€§**ï¼š

- â€‹`jax.numpy`â€‹ (jnp) å°½å¯èƒ½å¤åˆ»äº† `numpy` (np) çš„ APIã€‚
- JAX å¯ä»¥åœ¨è®¡ç®—ä¸­æ··åˆä½¿ç”¨ NumPy æ•°ç»„å’Œ JAX æ•°ç»„ã€‚å½“ä¸¤è€…è¿ç®—æ—¶ï¼ŒJAX ä¼šéšå¼è§¦å‘ `Host-to-Device`â€‹ çš„æ•°æ®ä¼ è¾“ï¼Œå°† NumPy æ•°ç»„æ¬è¿è‡³ GPUï¼Œç»“æœè¿”å›ä¸º `DeviceArray`ã€‚
- æ˜¾å¼ä¼ è¾“æ§åˆ¶ï¼š

  - â€‹`jax.device_put(x)`: Host $\rightarrow$ Device
  - â€‹`jax.device_get(x)`: Device $\rightarrow$ Host

```python
import jax
import jax.numpy as jnp
print("Using jax", jax.__version__)
a = jnp.zeros((2, 5), dtype=jnp.float32)
print(a)
b = jnp.arange(6)
print(b)
# [0 1 2 3 4 5]
b.__class__
# jaxlib.xla_extension.DeviceArray
b.device()
# GpuDevice(id=0, process_index=0)
b_cpu = jax.device_get(b)
print(b_cpu.__class__)
# <class 'numpy.ndarray'>
b_gpu = jax.device_put(b_cpu)
print(f'Device put: {b_gpu.__class__} on {b_gpu.device()}')
# Device put: <class 'jaxlib.xla_extension.DeviceArray'> on gpu:0
b_cpu + b_gpu
# DeviceArray([ 0,  2,  4,  6,  8, 10], dtype=int32)
jax.devices()
# [GpuDevice(id=0, process_index=0), GpuDevice(id=1, process_index=0)]
```

**æ‰§è¡Œæœºåˆ¶ï¼šå¼‚æ­¥è°ƒåº¦ (Asynchronous Dispatch)**

è¿™æ˜¯ä¸€ä¸ªéå¸¸å…³é”®çš„æ€§èƒ½ç‰¹æ€§ã€‚

- â€‹**ç°è±¡**â€‹ï¼šå½“ä½ æ‰§è¡Œ `c = jnp.matmul(a, b)`â€‹ æ—¶ï¼ŒPython è§£é‡Šå™¨ä¼š**ç«‹å³**è¿”å›ä¸€ä¸ª `c` çš„å¥æŸ„ï¼ˆFutureï¼‰ï¼Œè€Œä¸ä¼šç­‰å¾… GPU è®¡ç®—å®Œæˆã€‚
- â€‹**åŸç†**ï¼šJAX ç»´æŠ¤äº†ä¸€ä¸ªæŒ‡ä»¤é˜Ÿåˆ—ã€‚Python çº¿ç¨‹åªéœ€å°†æ“ä½œå‹å…¥é˜Ÿåˆ—å³å¯ç»§ç»­å‘ä¸‹æ‰§è¡Œã€‚è¿™æ„å‘³ç€ Python ä»£ç çš„æ‰§è¡Œé€šå¸¸é¢†å…ˆäº GPU çš„å®é™…è®¡ç®—ã€‚
- â€‹**åŒæ­¥ç‚¹ (Blocking)** â€‹ï¼šåªæœ‰å½“ä½ è¯•å›¾**è¯»å–**ç»“æœçš„å…·ä½“æ•°å€¼æ—¶ï¼ˆä¾‹å¦‚ `print(c)`â€‹ã€`np.array(c)`â€‹ æˆ–æ§åˆ¶æµåˆ¤æ–­ `if c > 0`ï¼‰ï¼ŒJAX æ‰ä¼šå¼ºåˆ¶é˜»å¡ Python çº¿ç¨‹ï¼Œç­‰å¾… GPU è®¡ç®—å®Œæˆå¹¶ä¼ å›æ•°æ®ã€‚
- â€‹**æ„ä¹‰**ï¼šè¿™æœ€å¤§åŒ–äº† CPUï¼ˆæŒ‡ä»¤åˆ†å‘ï¼‰å’Œ GPUï¼ˆå¤§è§„æ¨¡å¹¶è¡Œè®¡ç®—ï¼‰çš„é‡å æ‰§è¡Œæ—¶é—´ï¼Œéšè—äº† Python çš„è§£é‡Šå™¨å¼€é”€ã€‚

### ä¸å¯å˜å¼ é‡ (Immutable Tensors)

**èŒƒå¼çº¦æŸ**

- â€‹**JAX çº¦æŸ**â€‹ï¼šJAX æ•°ç»„æ˜¯**ä¸å¯å˜ (Immutable)**  çš„ã€‚
- â€‹**NumPy å¯¹æ¯”**â€‹ï¼šNumPy æ”¯æŒ In-place æ›´æ–°ï¼Œå³ `x[0] = 1` ä¼šç›´æ¥ä¿®æ”¹å†…å­˜åœ°å€ä¸­çš„å€¼ã€‚
- â€‹**JAX è¡Œä¸º**â€‹ï¼š`x[0] = 1`â€‹ åœ¨ JAX ä¸­ä¼šæŠ›å‡º `TypeError`ã€‚

**åŸå› ï¼šå‡½æ•°å¼ç¼–ç¨‹ä¸è¿½è¸ª (Tracing)**

JAX çš„æ ¸å¿ƒèƒ½åŠ›ä¾èµ–äºå¯¹çº¯å‡½æ•°ï¼ˆPure Functionsï¼‰çš„è¿½è¸ªï¼ˆTracingï¼‰æ¥æ„å»ºè®¡ç®—å›¾ï¼ˆJaxprï¼‰ã€‚

- å¦‚æœå…è®¸ In-place ä¿®æ”¹ï¼Œä¼šå¼•å…¥**å‰¯ä½œç”¨ (Side Effects)** ï¼Œä½¿å¾—è¾“å…¥å˜é‡çš„çŠ¶æ€åœ¨å‡½æ•°æ‰§è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿæ”¹å˜ã€‚è¿™ä¼šå¯¼è‡´ç¼–è¯‘å™¨ï¼ˆXLAï¼‰éš¾ä»¥åˆ†ææ•°æ®ä¾èµ–å…³ç³»ï¼Œä»è€Œä½¿å¾—æ¢¯åº¦è®¡ç®—å’Œç®—å­èåˆå˜å¾—æåº¦å¤æ‚ç”šè‡³ä¸å¯è¡Œã€‚

```python
y = x.at[0].set(1)
# x ä¿æŒåŸå€¼
# y æ˜¯æ–°å€¼
b_new = b.at[0].set(1)
print('Original array:', b)
print('Changed array:', b_new)
# Original array: [0 1 2 3 4 5]
# Changed array: [1 1 2 3 4 5]
```

**ç¼–è¯‘ä¼˜åŒ–**ï¼šä½ å¯èƒ½ä¼šæ‹…å¿ƒè¿™å°±æ„å‘³ç€å¤§é‡çš„æ˜¾å­˜æ‹·è´ï¼ˆCopy overheadï¼‰ã€‚ä½†åœ¨ `jax.jit`â€‹ ç¼–è¯‘åï¼ŒXLA ç¼–è¯‘å™¨è¶³å¤Ÿæ™ºèƒ½ï¼Œå®ƒä¼šåˆ†ææ•°æ®æµã€‚å¦‚æœåŸæ•°ç»„ `x`â€‹ åœ¨åç»­ä¸å†è¢«ä½¿ç”¨ï¼ŒXLA ä¼šåœ¨åº•å±‚çš„ GPU Kernel ä¸­å°†å…¶ä¼˜åŒ–ä¸º**åŸåœ°ä¿®æ”¹ (In-place mutation)** ã€‚å› æ­¤ï¼Œè¿™ç§å†™æ³•åœ¨ç¼–è¯‘åçš„æ€§èƒ½ä¸åŸåœ°ä¿®æ”¹æ˜¯ä¸€è‡´çš„ã€‚

### æ— çŠ¶æ€ä¼ªéšæœºæ•°ç”Ÿæˆ (Stateless PRNG)

**ä¼ ç»Ÿæ¨¡å¼ï¼šæœ‰çŠ¶æ€ (Stateful)**

- â€‹**NumPy/PyTorch**â€‹ï¼šç»´æŠ¤ä¸€ä¸ª**å…¨å±€**çš„éšæœºæ•°ç”Ÿæˆå™¨çŠ¶æ€ï¼ˆGlobal Contextï¼‰ã€‚
- â€‹**æ“ä½œ**â€‹ï¼šè°ƒç”¨ `torch.randn()` æ—¶ï¼Œå‡½æ•°å†…éƒ¨ä¼šéšå¼åœ°è¯»å–å¹¶æ›´æ–°è¿™ä¸ªå…¨å±€çŠ¶æ€ã€‚
- â€‹**é—®é¢˜**ï¼šè¿™ç§éšå¼çš„çŠ¶æ€æ›´æ–°è¿èƒŒäº†â€œçº¯å‡½æ•°â€åŸåˆ™ã€‚åœ¨å¹¶è¡Œè®¡ç®—æˆ–åˆ†å¸ƒå¼è®­ç»ƒä¸­ï¼Œéšå¼çŠ¶æ€ä¼šå¯¼è‡´éšæœºæ•°åºåˆ—éš¾ä»¥å¤ç°ï¼Œä¸”éš¾ä»¥è¿›è¡Œå¹¶è¡Œçš„ç‹¬ç«‹é‡‡æ ·ã€‚

**JAX æ¨¡å¼ï¼šæ— çŠ¶æ€ (Stateless)**

- â€‹**æ ¸å¿ƒåŸåˆ™**ï¼šéšæœºæ•°ç”Ÿæˆæ˜¯ä¸€ä¸ªç¡®å®šæ€§çš„å‡½æ•°ï¼š$Output = f(Key, Structure)$ã€‚åªè¦ Key ä¸å˜ï¼Œç”Ÿæˆçš„éšæœºæ•°æ°¸è¿œä¸å˜ã€‚
- **PRNG Key**ï¼šä½ å¿…é¡»æ˜¾å¼åœ°åˆ›å»ºå’Œä¼ é€’éšæœºæ•°çŠ¶æ€ï¼ˆKeyï¼‰ã€‚

```python
key = jax.random.PRNGKey(42)
```

**å…³é”®æ“ä½œï¼šSplit**

ä¸ºäº†åœ¨ç”Ÿæˆéšæœºæ•°çš„åŒæ—¶è·å¾—ä¸‹ä¸€ä¸ªå¯ç”¨çš„çŠ¶æ€ï¼ŒJAX å¼•å…¥äº† `split` æœºåˆ¶ã€‚

- â€‹**å•æ¬¡ä½¿ç”¨åŸåˆ™**ï¼šä¸€ä¸ª Key åªèƒ½ç”¨äºä¸€æ¬¡éšæœºé‡‡æ ·æ“ä½œï¼ˆæˆ–è€…è¯´ï¼ŒåŒä¸€ä¸ª Key ç”¨äºåŒä¸€æ“ä½œä¼šå¾—åˆ°å®Œå…¨ç›¸åŒçš„ç»“æœï¼‰ã€‚
- **Split æµç¨‹**ï¼šå°†ä¸€ä¸ªæ—§çš„ Key åˆ†è£‚ä¸ºå¤šä¸ªæ–°çš„ã€ç»Ÿè®¡å­¦ä¸Šç‹¬ç«‹çš„ Keyã€‚

```python
rng, subkey = jax.random.split(rng)
# subkey -> ç”¨äºå½“å‰çš„éšæœºæ“ä½œ (å¦‚ dropout, init weights)
# rng    -> ä¼ é€’ç»™åç»­çš„ä»£ç å—ç»§ç»­ split
```

å›¾ç¤ºé€»è¾‘ï¼š

$$
Key_t \xrightarrow{\text{split}} (Key_{t+1}, SubKey_{task})
$$

ä½ æ€»æ˜¯ä¿ç•™ä¸€ä¸ªä¸» Key (rng) ç”¨äºå‘ä¸‹ä¼ é€’ï¼Œè€Œâ€œæ¶ˆè€—â€æ‰åˆ†è£‚å‡ºæ¥çš„ subkey ç”¨äºå…·ä½“çš„è®¡ç®—ä»»åŠ¡ã€‚

```python
rng = jax.random.PRNGKey(42)
# A non-desirable way of generating pseudo-random numbers...
jax_random_number_1 = jax.random.normal(rng)
jax_random_number_2 = jax.random.normal(rng)
print('JAX - Random number 1:', jax_random_number_1)
print('JAX - Random number 2:', jax_random_number_2)

# Typical random numbers in NumPy
np.random.seed(42)
np_random_number_1 = np.random.normal()
np_random_number_2 = np.random.normal()
print('NumPy - Random number 1:', np_random_number_1)
print('NumPy - Random number 2:', np_random_number_2)
```

JAX - Random number 1: -0.18471177  
JAX - Random number 2: -0.18471177  
NumPy - Random number 1: 0.4967141530112327  
NumPy - Random number 2: -0.13826430117118466

```python
rng, subkey1, subkey2 = jax.random.split(rng, num=3)  # We create 3 new keys
jax_random_number_1 = jax.random.normal(subkey1)
jax_random_number_2 = jax.random.normal(subkey2)
print('JAX new - Random number 1:', jax_random_number_1)
print('JAX new - Random number 2:', jax_random_number_2)
```

JAX new - Random number 1: 0.107961535  
JAX new - Random number 2: -1.2226542

â€

## Function transformations with Jaxpr

JAX ä¸ä»…ä»…æ˜¯ä¸€ä¸ªæ•°å€¼è®¡ç®—åº“ï¼Œæ›´æ˜¯ä¸€ä¸ª**ç¼–è¯‘å™¨å‰ç«¯**ã€‚å®ƒé€šè¿‡ä¸€ç§ç§°ä¸º **Tracingï¼ˆè¿½è¸ªï¼‰**  çš„æœºåˆ¶ï¼Œå°† Python å‡½æ•°è½¬æ¢ä¸ºä¸€ç§ä¸­é—´è¡¨ç¤ºï¼ˆIntermediate Representation, IRï¼‰ï¼Œå³ **Jaxpr (JAX Expression)** ã€‚åŸºäºè¿™ä¸ª IRï¼ŒJAX å®ç°äº†è‡ªåŠ¨å¾®åˆ†ã€ç¼–è¯‘ä¼˜åŒ–ç­‰åŠŸèƒ½ã€‚

### ä¸­é—´è¡¨ç¤ºï¼šJaxpr (JAX Expression)

![image](JAX/image-20251130152627-18phbyj.png)

- â€‹**å·¦ä¾§ (Frontend)** ï¼šæœ‰ $M$ ç§æºè¯­è¨€ (C, C++, Go, Rust...)ã€‚
- â€‹**å³ä¾§ (Backend)** ï¼šæœ‰ $N$ ç§ç¡¬ä»¶æ¶æ„ (x86, ARM, RISC-V...)ã€‚
- â€‹**ä¸­é—´ (IR)** â€‹ï¼šåªæœ‰ **1** ç§æ ¸å¿ƒè¡¨è¾¾ (LLVM IR)ã€‚

å¦‚æœæ²¡æœ‰ IR,ä½ éœ€è¦ä¸ºæ¯ä¸€å¯¹â€œè¯­è¨€-ç¡¬ä»¶â€å†™ä¸€ä¸ªç¼–è¯‘å™¨ï¼ˆæ¯”å¦‚ C-to-x86, Go-to-ARMï¼‰ã€‚ä½ éœ€è¦å†™ \$M \\times N\$ ä¸ªç¼–è¯‘å™¨ã€‚è¿™æ˜¯ä¸€åœºå·¥ç¨‹ç¾éš¾ã€‚

**æœ‰äº† IRï¼š**

- **å‰ç«¯**åªéœ€è´Ÿè´£å°†æºç ç¿»è¯‘æˆ IR ($M$ ä¸ªå‰ç«¯)ã€‚
- **åç«¯**åªéœ€è´Ÿè´£å°† IR ç¿»è¯‘æˆæœºå™¨ç  ($N$ ä¸ªåç«¯)ã€‚
- â€‹**å¤æ‚åº¦é™ä½**ï¼šä» $M \times N$ é™åˆ°äº† $M + N$ã€‚

<u>æ˜ å°„åˆ° JAXï¼š</u>

- â€‹**Frontend**: Python (ç”¨æˆ·å†™çš„é«˜çº§ä»£ç )ã€‚
- â€‹**IR**â€‹: **Jaxpr** (ä»¥åŠæ›´åº•å±‚çš„ XLA HLO)ã€‚
- â€‹**Backend**: NVIDIA GPU, Google TPU, CPUã€‚
- â€‹**æ„ä¹‰**ï¼šJAX ä¸éœ€è¦ä¸“é—¨ä¸º GPU å†™ä¸€å¥— Python è§£é‡Šå™¨ï¼Œä¹Ÿä¸ç”¨ä¸º TPU å†™å¦ä¸€å¥—ã€‚å®ƒåªéœ€è¦æŠŠ Python å˜æˆ Jaxprï¼Œå‰©ä¸‹çš„äº¤ç»™ XLA ç¼–è¯‘å™¨ï¼ˆåç«¯ï¼‰å»å¤„ç†å¦‚ä½•æ¬è¿åˆ°ä¸åŒç¡¬ä»¶ä¸Šã€‚

â€‹<kbd>Jaxpr vs. LLVM IRï¼šä¸åŒå±‚çº§çš„æŠ½è±¡</kbd>

**LLVM IR (é€šç”¨è®¡ç®—çš„ IR)**

- â€‹**æŠ½è±¡å¯¹è±¡**ï¼šå¯„å­˜å™¨ã€å†…å­˜åœ°å€ã€åŸºç¡€ç®—æœ¯æŒ‡ä»¤ï¼ˆåŠ å‡ä¹˜é™¤ï¼‰ã€è·³è½¬æŒ‡ä»¤ã€‚
- â€‹**è®¾è®¡ç›®æ ‡**ï¼šä¼˜åŒ–æ ‡é‡è®¡ç®—ã€æ§åˆ¶æµï¼ˆif/else/loopï¼‰ã€‚
- â€‹**å›¾ç¤ºå¯¹åº”**ï¼šå›¾ä¸­ä¸­é—´æ¡†å†…çš„ "LLVM IR"ã€‚

**Jaxpr (æ·±åº¦å­¦ä¹ çš„ IR)**

- â€‹**æŠ½è±¡å¯¹è±¡**â€‹ï¼šâ€‹**å¼ é‡ (Tensors)** ã€é«˜é˜¶ç®—å­ (MatMul, Conv)ã€è‡ªåŠ¨å¾®åˆ†åŸè¯­ã€‚
- â€‹**è®¾è®¡ç›®æ ‡**ï¼šä¿ç•™æ•°æ®æµç»“æ„ï¼Œæ–¹ä¾¿è¿›è¡Œæ•°å­¦å˜æ¢ï¼ˆå¾®åˆ†ï¼‰å’Œå¤§è§„æ¨¡å¹¶è¡ŒåŒ–ã€‚
- â€‹**æœ¬è´¨**â€‹ï¼šJaxpr æ˜¯ä¸€ç§ â€‹**Domain Specific IR (DSIR)** ã€‚

å…³é”®åŒºåˆ«ï¼š

å¦‚æœä½ åœ¨ LLVM IR å±‚é¢åšâ€œè‡ªåŠ¨å¾®åˆ†â€ï¼Œä¼šæå…¶ç—›è‹¦ï¼Œå› ä¸ºé‚£é‡Œåªæœ‰å¯„å­˜å™¨å’ŒæŒ‡é’ˆï¼Œä¸¢å¤±äº†â€œçŸ©é˜µä¹˜æ³•â€è¿™æ ·çš„æ•°å­¦è¯­ä¹‰ã€‚

è€Œåœ¨ Jaxpr å±‚é¢ï¼Œç³»ç»Ÿæ¸…æ¥šåœ°çŸ¥é“ dot\_general æ˜¯çŸ©é˜µä¹˜æ³•ï¼Œå› æ­¤å¯ä»¥è½»æ¾åº”ç”¨ $\frac{\partial (WX)}{\partial W} = X^T$ è¿™æ ·çš„æ•°å­¦è§„åˆ™ã€‚

â€‹<kbd>Optimization Passes (ä¼˜åŒ–é)</kbd>

å›¾ç‰‡ä¸­é—´æœ‰ä¸€ä¸ªæ ¸å¿ƒæ¨¡å—ï¼š**LLVM Optimizer** å’Œä¸‹é¢çš„ â€‹**Pass ->**  **Pass ->**  **Pass**ã€‚

**ä»€ä¹ˆæ˜¯ Passï¼Ÿ** Pass æ˜¯ç¼–è¯‘å™¨å¯¹ IR è¿›è¡Œçš„ä¸€æ¬¡å®Œæ•´æ‰«æå’Œå˜æ¢ã€‚æ¯æ¬¡ Pass éƒ½åœ¨ä¸æ”¹å˜ç¨‹åºè¯­ä¹‰çš„å‰æä¸‹ï¼Œè®©ä»£ç å˜å¾—æ›´é«˜æ•ˆã€‚

**åœ¨ LLVM ä¸­ï¼š**

- â€‹*Dead Code Elimination Pass*: åˆ é™¤æ°¸è¿œä¸ä¼šæ‰§è¡Œçš„ä»£ç ã€‚
- â€‹*Loop Unrolling Pass*: æŠŠå¾ªç¯å±•å¼€ã€‚

**åœ¨ JAX (åŸºäº Jaxpr/XLA) ä¸­ï¼š** å½“ä½ è°ƒç”¨ `@jax.jit`â€‹ æ—¶ï¼ŒJAX ä¼šç”Ÿæˆ Jaxprï¼Œç„¶å XLA ç¼–è¯‘å™¨ä¼šå¯¹è¿™ä¸ª IR æ‰§è¡Œä¸€ç³»åˆ—é’ˆå¯¹æ·±åº¦å­¦ä¹ çš„ â€‹**Pass**ï¼š

1. â€‹**Operator Fusion Pass (ç®—å­èåˆ)** ï¼š

    - â€‹*è¾“å…¥ IR*â€‹: `Load A`â€‹ -\> `Load B`â€‹ -\> `Add`â€‹ -\> `Store Temp`â€‹ -\> `Load Temp`â€‹ -\> `Relu`â€‹ -\> `Store C`
    - â€‹*ä¼˜åŒ–å IR*â€‹: `Load A, B`â€‹ -\> `Add & Relu`â€‹ -\> `Store C` (èåˆä¸ºä¸€ä¸ª Kernelï¼Œæ¶ˆç­ä¸­é—´å†…å­˜è¯»å†™)ã€‚
2. â€‹**Buffer Donation Pass**:

    - åˆ†æ IR ä¸­çš„å˜é‡ç”Ÿå‘½å‘¨æœŸï¼Œå¤ç”¨æ˜¾å­˜ï¼Œå‡å°‘å†…å­˜ç”³è¯·å¼€é”€ã€‚
3. â€‹**CSE (Common Subexpression Elimination)** :

    - å‘ç° IR é‡Œç®—äº†ä¸¤æ¬¡ä¸€æ ·çš„ `a * b`ï¼Œæ”¹æˆåªç®—ä¸€æ¬¡ã€‚

â€
è¯·è®°ä½å…¬å¼ $y = \frac{1}{|x|} \sum_{i} \left[ (x_i + 2)^2 + 3 \right]$ï¼Œåé¢ä»£ç éƒ½æºäºå®ƒã€‚
```python
def simple_graph(x):
    x = x + 2
    x = x ** 2
    x = x + 3
    y = x.mean()
    return y

inp = jnp.arange(3, dtype=jnp.float32)
print('Input', inp)
print('Output', simple_graph(inp))
# Input [0. 1. 2.]
# Output 12.666667
jax.make_jaxpr(simple_graph)(inp)
'''
{ lambda ; a:f32[3]. let
    b:f32[3] = add a 2.0
    c:f32[3] = integer_pow[y=2] b
    d:f32[3] = add c 3.0
    e:f32[] = reduce_sum[axes=(0,)] d  //æ²¿ç€ç»´åº¦0è§„èŒƒæ±‚å’Œ
    f:f32[] = div e 3.0  //é™¤ä»¥3
  in (f,) }
'''
```

A jaxpr representation follows the structure:

```plaintext
jaxpr ::= { lambda Var* ; Var+.
            let Eqn*
            in  [Expr+] }
```

**çº¯å‡½æ•°çº¦æŸ (Purity Constraint)**

æ•™ç¨‹ä¸­çš„ `norm(x)` ç¤ºä¾‹å±•ç¤ºäº†éçº¯å‡½æ•°ï¼ˆImpure Functionï¼‰åœ¨ Tracing æœºåˆ¶ä¸‹çš„è¡Œä¸ºï¼š

- â€‹**ä»£ç **â€‹ï¼š`global_list.append(x)`
- â€‹**ç°è±¡**ï¼šç”Ÿæˆçš„ Jaxpr ä¸­å®Œå…¨ä¸¢å¤±äº†è¿™ä¸€è¡Œæ“ä½œã€‚
- â€‹**åŸå› **â€‹ï¼š`list.append`â€‹ æ˜¯ Python çš„å‰¯ä½œç”¨æ“ä½œï¼Œä¸å±äº JAX çš„åŸè¯­ã€‚Tracer åœ¨è¿½è¸ªè¿‡ç¨‹ä¸­è™½ç„¶æ‰§è¡Œäº†è¿™è¡Œ Python ä»£ç ï¼Œä½†å› ä¸ºæ²¡æœ‰äº§ç”Ÿ JAX çš„æ•°æ®ä¾èµ–ï¼Œè¿™ä¸ªæ“ä½œâ€‹**ä¸ä¼šè¢«è®°å½•åœ¨è®¡ç®—å›¾ä¸­**ã€‚
- **ç»“è®º**ï¼šJAX å˜æ¢åªå¯¹**æ•°æ®æµ (Data Flow)æ•æ„Ÿï¼Œå¯¹ Python çš„æ§åˆ¶æµ (Control Flow)å’Œå‰¯ä½œç”¨ (Side Effects)**  åªæœ‰åœ¨å®ƒä»¬ç›´æ¥å½±å“ JAX æ•°ç»„è®¡ç®—æ—¶æ‰ä¼šè¢«æ•æ‰ï¼ˆ<u>æ§åˆ¶æµéœ€è¦ç”¨ </u>â€‹<u>â€‹`jax.lax.cond`â€‹</u>â€‹<u> ç­‰åŸè¯­æ›¿ä»£</u>ï¼‰ã€‚

```python
global_list = []

# Invalid function with side-effect
def norm(x):
    global_list.append(x)
    x = x ** 2
    n = x.sum()
    n = jnp.sqrt(n)
    return n

jax.make_jaxpr(norm)(inp)
'''
{ lambda ; a:f32[3]. let
    b:f32[3] = integer_pow[y=2] a
    c:f32[] = reduce_sum[axes=(0,)] b
    d:f32[] = sqrt c
  in (d,) }
'''
```

### Automatic differentiation

å˜æ¢æ˜ å°„ï¼šjax.grad æ˜¯ä¸€ä¸ªé«˜é˜¶å‡½æ•°ï¼Œå…¶æ˜ å°„å…³ç³»ä¸ºï¼š

$$
f: \mathbb{R}^n \rightarrow \mathbb{R} \implies \nabla f: \mathbb{R}^n \rightarrow \mathbb{R}^n
$$

è¾“å…¥åŸå‡½æ•°ï¼Œè¿”å›æ¢¯åº¦å‡½æ•°ã€‚(gradè¾“å‡ºçš„æ¯ä¸€ç»´å¯¹åº”é‚£ä¸ªå‘é‡çš„åå¯¼æ•°)

```python
grad_function = jax.grad(simple_graph)
gradients = grad_function(inp)
print('Gradient', gradients)
```

åŸå‡½æ•°ï¼š$y = \frac{1}{3} \sum ((x+2)^2 + 3)$

æ ¹æ®é“¾å¼æ³•åˆ™ï¼ˆChain Ruleï¼‰ï¼š

$$
\frac{\partial y}{\partial x} = \underbrace{\frac{\partial y}{\partial \text{sum}}}_{\text{Meançš„å¯¼æ•°}} \cdot \underbrace{\frac{\partial \text{sum}}{\partial \text{square}}}_{\text{Sumçš„å¯¼æ•°}} \cdot \underbrace{\frac{\partial \text{square}}{\partial (x+2)}}_{\text{Squareçš„å¯¼æ•°}}
$$

{ lambda ; a:f32[3]. let

b:f32[3] \= add a 2.0

c:f32[3] \= integer\_pow[y\=2] b

d:f32[3] \= integer\_pow[y\=1] b

e:f32[3] \= mul 2.0 d

f:f32[3] \= add c 3.0

g:f32[] \= reduce\_sum[axes\=(0,)] f

\_:f32[] \= div g 3.0

h:f32[] \= div 1.0 3.0

i:f32[3] \= broadcast\_in\_dim[broadcast\_dimensions\=() shape\=(3,)] h

j:f32[3] \= mul i e

in (j,) }

â€

ä¸ºäº†è®¡ç®—æ¢¯åº¦ï¼Œå¾€å¾€éœ€è¦ç”¨åˆ°å‰å‘ä¼ æ’­çš„ä¸­é—´å€¼ã€‚

- â€‹`b:f32[3] = add a 2.0`: è®¡ç®— $x+2$ã€‚
- â€‹`c:f32[3] = integer_pow[y=2] b`: è®¡ç®— $(x+2)^2$ã€‚
- â€‹`f:f32[3] = add c 3.0`: è®¡ç®— $(x+2)^2 + 3$ã€‚

â€

è¿™é‡Œå¯¹åº”æ•°å­¦ä¸­çš„ $\frac{d}{dx}(u^2) = 2u$ã€‚

- â€‹`d:f32[3] = integer_pow[y=1] b`:

  - è¿™æ˜¯è®¡ç®— $b^1$ï¼Œä¹Ÿå°±æ˜¯ $x+2$ æœ¬èº«ã€‚
  - *ä¸ºä»€ä¹ˆä¼šæœ‰è¿™è¡Œï¼Ÿ*  å› ä¸ºå¹‚å‡½æ•°çš„æ±‚å¯¼é€šç”¨å…¬å¼æ˜¯ $n \cdot u^{n-1}$ã€‚è¿™é‡Œ $n=2$ï¼Œæ‰€ä»¥å®ƒæ˜¯ç®— $u^{2-1}$ã€‚
- â€‹`e:f32[3] = mul 2.0 d`:

  - è¿™æ˜¯è®¡ç®— $2.0 \times d$ã€‚
  - **ç»“è®º**ï¼š`e` ç°åœ¨çš„æ•°å€¼å°±æ˜¯ **$2(x+2)$**ã€‚è¿™æ˜¯ $(x+2)^2$ è¿™ä¸€å±‚çš„å±€éƒ¨æ¢¯åº¦ã€‚

â€

- â€‹`g:f32[] = reduce_sum[axes=(0,)] f`: å¯¹å‰å‘ç»“æœæ±‚å’Œã€‚
- â€‹`_:f32[] = div g 3.0`: æ±‚å¹³å‡å€¼ã€‚

  - â€‹**æ³¨æ„å˜é‡å**  **â€‹`_`â€‹** ï¼šåœ¨ Python å’Œå¾ˆå¤š IR ä¸­ï¼Œä¸‹åˆ’çº¿è¡¨ç¤ºâ€œæˆ‘ä¸å…³å¿ƒè¿™ä¸ªç»“æœâ€ã€‚
  - å› ä¸ºä½ è°ƒç”¨çš„æ˜¯ `jax.grad`â€‹ è€Œä¸æ˜¯ `jax.value_and_grad`â€‹ï¼Œä½ åªæƒ³è¦**æ¢¯åº¦**ã€‚è™½ç„¶ä¸ºäº†è¿½è¸ªå›¾ç»“æ„ï¼ŒJAX ç”Ÿæˆäº†è®¡ç®— Loss çš„æ­¥éª¤ï¼Œä½†å®ƒå¹¶æœªå°†å…¶ä½œä¸ºæœ€ç»ˆè¾“å‡ºè¿”å›ã€‚

â€

- â€‹`h:f32[] = div 1.0 3.0`:

  - è®¡ç®—æ ‡é‡ $1.0 / 3.0 = 0.333...$ã€‚è¿™å°±æ˜¯ $\frac{1}{N}$ã€‚
- â€‹`i:f32[3] = broadcast_in_dim[... shape=(3,)] h`:

  - â€‹**å¹¿æ’­ (Broadcast)** ï¼šå°†æ ‡é‡ $0.333$ å¤åˆ¶æ‰©å±•æˆä¸€ä¸ªé•¿åº¦ä¸º 3 çš„å‘é‡ `[0.333, 0.333, 0.333]`ã€‚
  - è¿™ä»£è¡¨äº† Loss å¯¹æ±‚å’Œå±‚ä¹‹å‰çš„æ¯ä¸€ä¸ªå…ƒç´ çš„æ¢¯åº¦æƒé‡éƒ½æ˜¯ $1/3$ã€‚

â€

- â€‹`j:f32[3] = mul i e`:

  - â€‹**â€‹`i`â€‹** æ˜¯ Mean å±‚çš„æ¢¯åº¦ ($\frac{1}{3}$).
  - â€‹**â€‹`e`â€‹** æ˜¯ Square å±‚çš„æ¢¯åº¦ ($2(x+2)$).
  - å®ƒä»¬ç›¸ä¹˜ï¼š$\frac{1}{3} \times 2(x+2) = \frac{2}{3}(x+2)$ã€‚

â€

Jaxpr å®é™…ä¸Šæ˜¯åœ¨æ‰§è¡Œä»¥ä¸‹æ•°å­¦è¿ç®—ï¼š

$$
\nabla = \left( \underbrace{\text{broadcast}(\frac{1}{3})}_{i} \right) \times \left( \underbrace{2 \cdot (x+2)}_{e} \right)
$$

JAX çš„å·¥ä½œæµï¼šå®ƒé€šè¿‡ Trace æ‹¿åˆ°äº†è®¡ç®—å›¾ï¼Œç„¶ååº”ç”¨å¾®ç§¯åˆ†è§„åˆ™ï¼ˆAdjoint Logicï¼‰ï¼Œ**ç”Ÿæˆäº†ä¸€ä¸ªæ–°çš„è®¡ç®—å›¾**ï¼ˆå°±æ˜¯ä½ çœ‹åˆ°çš„è¿™ä¸ª Jaxprï¼‰æ¥ä¸“é—¨è®¡ç®—æ¢¯åº¦ã€‚

â€‹<kbd>Value and Grad</kbd>

- â€‹**API**â€‹: `jax.value_and_grad(f)`
- â€‹**ç›®çš„**ï¼šåœ¨æ·±åº¦å­¦ä¹ è®­ç»ƒä¸­ï¼Œæˆ‘ä»¬é€šå¸¸éœ€è¦åŒæ—¶è·å– Loss å€¼ï¼ˆç”¨äºæ—¥å¿—è®°å½•ï¼‰å’Œ Gradientsï¼ˆç”¨äºå‚æ•°æ›´æ–°ï¼‰ã€‚
- **æ•ˆç‡**ï¼šè¯¥å‡½æ•°é€šè¿‡ä¸€æ¬¡å‰å‘ä¼ æ’­å’Œä¸€æ¬¡åå‘ä¼ æ’­åŒæ—¶è®¡ç®— $f(x)$ å’Œ $\nabla f(x)$ï¼Œé¿å…äº†åˆ†åˆ«è°ƒç”¨ `f(x)`â€‹ å’Œ `jax.grad(f)(x)` å¯¼è‡´çš„é‡å¤å‰å‘è®¡ç®—å¼€é”€ã€‚

```python
val_grad_function = jax.value_and_grad(simple_graph)
val_grad_function(inp)

"""
(DeviceArray(12.666667, dtype=float32),
 DeviceArray([1.3333334, 2.       , 2.6666667], dtype=float32))
"""
```

â€‹<kbd>PyTrees</kbd>

- â€‹**å®šä¹‰**â€‹ï¼šJAX å°†åµŒå¥—çš„å®¹å™¨ç»“æ„ï¼ˆå¦‚ `dict`â€‹ of `list`â€‹ of `arrays`â€‹ï¼‰ç§°ä¸º â€‹**PyTree**ã€‚
- â€‹**åº”ç”¨**â€‹ï¼šç¥ç»ç½‘ç»œçš„å‚æ•°é€šå¸¸ä»¥å­—å…¸å½¢å¼ç»„ç»‡ï¼ˆå¦‚ `{'layer1': {'w': ..., 'b': ...}}`â€‹ï¼‰ã€‚`jax.grad` èƒ½å¤Ÿè‡ªåŠ¨å¤„ç†è¿™ç§ç»“æ„ï¼Œè¿”å›ä¸€ä¸ªä¸è¾“å…¥å‚æ•°ç»“æ„å®Œå…¨ä¸€è‡´çš„æ¢¯åº¦ PyTreeã€‚

### Just-In-Time Compilation

**XLA (Accelerated Linear Algebra)**

â€‹`jax.jit` æ˜¯è¿æ¥ JAX å‰ç«¯ä¸ XLA åç«¯çš„æ¡¥æ¢ã€‚

- â€‹**æµç¨‹**ï¼šPython Function $\xrightarrow{\text{Trace}}$ Jaxpr $\xrightarrow{\text{Lowering}}$ HLO (High Level Optimizer) IR $\xrightarrow{\text{Compile}}$ Optimized Kernel (Machine Code)ã€‚
- â€‹**ç®—å­èåˆ (Kernel Fusion)** â€‹ï¼šè¿™æ˜¯ XLA å¸¦æ¥çš„æœ€å¤§æ€§èƒ½æ”¶ç›Šã€‚å®ƒå°†å¤šä¸ªå†…å­˜å¯†é›†å‹æ“ä½œï¼ˆå¦‚ `add`â€‹, `mul`ï¼‰èåˆä¸ºä¸€ä¸ª GPU Kernelï¼Œæå¤§å‡å°‘äº† HBMï¼ˆé«˜å¸¦å®½å†…å­˜ï¼‰çš„è¯»å†™æ¬¡æ•°ã€‚

**å¤šæ€æ€§ä¸é‡ç¼–è¯‘ (Polymorphism & Re-compilation)**

- â€‹**é™æ€å½¢çŠ¶é™åˆ¶**ï¼šç¼–è¯‘åçš„ Kernel æ˜¯é’ˆå¯¹ç‰¹å®šè¾“å…¥å½¢çŠ¶ï¼ˆInput Shapeï¼‰ç¡¬ç¼–ç çš„ã€‚
- â€‹**è§¦å‘æœºåˆ¶**ï¼š

  1. ç¬¬ä¸€æ¬¡è°ƒç”¨ `jitted_function` æ—¶ï¼Œè§¦å‘ Tracing å’Œ Compilationï¼ˆè€—æ—¶è¾ƒé•¿ï¼‰ã€‚
  2. åç»­è°ƒç”¨è‹¥è¾“å…¥å½¢çŠ¶ç›¸åŒï¼Œç›´æ¥è°ƒç”¨ç¼“å­˜çš„ Kernelï¼ˆæå¿«ï¼‰ã€‚
  3. è‹¥è¾“å…¥å½¢çŠ¶æ”¹å˜ï¼ŒJAX å¿…é¡»â€‹**é‡æ–°ç¼–è¯‘**ã€‚å› æ­¤ï¼Œåœ¨å¤„ç†å˜é•¿åºåˆ—ï¼ˆå¦‚ NLPï¼‰æ—¶ï¼Œé€šå¸¸éœ€è¦ä½¿ç”¨ Padding å°†è¾“å…¥å›ºå®šä¸ºæ ‡å‡†é•¿åº¦ï¼Œä»¥é¿å…é¢‘ç¹çš„é‡ç¼–è¯‘å¼€é”€ã€‚

ç”±äº JAX çš„**å¼‚æ­¥è°ƒåº¦**æœºåˆ¶ï¼ŒPython ç«¯çš„è®¡æ—¶å™¨ä¼šåœ¨æŒ‡ä»¤åˆ†å‘å®Œæˆåç«‹å³åœæ­¢ï¼Œè€Œä¸æ˜¯ç­‰å¾… GPU è®¡ç®—ç»“æŸã€‚

- â€‹**è§£å†³æ–¹æ¡ˆ**â€‹ï¼šåœ¨æµ‹é€Ÿæ—¶å¿…é¡»è°ƒç”¨ `.block_until_ready()`ï¼Œå¼ºåˆ¶åŒæ­¥ CPU å’Œ GPUï¼Œä»¥è·å–çœŸå®çš„æ‰§è¡Œæ—¶é—´ã€‚
- **ç»“æœåˆ†æ**ï¼šæˆªå›¾æ˜¾ç¤ºï¼ŒJIT ç¼–è¯‘åçš„å‡½æ•°æ¯”åŸ Python å‡½æ•°å¿«äº†çº¦ **10-15å€**ï¼ˆ$598\mu s$ vs $19.5\mu s$ï¼‰ï¼Œä¸”æ¢¯åº¦è®¡ç®—çš„åŠ é€Ÿæ•ˆæœæ›´ä¸ºæ˜¾è‘—ã€‚

```python
jitted_function = jax.jit(simple_graph)
# Create a new random subkey for generating new random values
rng, normal_rng = jax.random.split(rng)
large_input = jax.random.normal(normal_rng, (1000,))
# Run the jitted function once to start compilation
_ = jitted_function(large_input)

%%timeit
simple_graph(large_input).block_until_ready()
# 598 Âµs Â± 104 Âµs per loop (mean Â± std. dev. of 7 runs, 1 loop each)
%%timeit
jitted_function(large_input).block_until_ready()
# 19.5 Âµs Â± 52.8 ns per loop (mean Â± std. dev. of 7 runs, 100000 loops each)
```

â€

## Implementing a Neural Network with Flax

### The Model

â€‹<kbd>æœ‰çŠ¶æ€ vs æ— çŠ¶æ€ (The Mental Model Shift)</kbd>

è¿™æ˜¯è¿™ä¸€èŠ‚æœ€è®© PyTorch ç”¨æˆ·â€œæŠ“ç‹‚â€çš„åœ°æ–¹ï¼Œå¿…é¡»é¦–å…ˆå»ºç«‹è¿™ä¸ªæ¦‚å¿µï¼š

- â€‹**PyTorch (**â€‹**â€‹`torch.nn.Module`â€‹**â€‹ **)** â€‹: æ˜¯â€‹**æœ‰çŠ¶æ€çš„ (Stateful)** ã€‚

  - å½“ä½ å®ä¾‹åŒ– `model = Net()`â€‹ æ—¶ï¼Œ`model`â€‹ å¯¹è±¡å†…éƒ¨**æŒæœ‰**äº†æ‰€æœ‰çš„æƒé‡ï¼ˆWeightsï¼‰å’Œåç½®ï¼ˆBiasesï¼‰ã€‚
  - ä½ è°ƒç”¨ `model(x)` æ—¶ï¼Œå®ƒè‡ªåŠ¨ä½¿ç”¨å†…éƒ¨å­˜çš„æƒé‡ã€‚
- â€‹**Flax (**â€‹**â€‹`flax.linen.Module`â€‹**â€‹ **)** â€‹: æ˜¯â€‹**æ— çŠ¶æ€çš„ (Stateless)** ã€‚

  - Flax çš„ `model`â€‹ å¯¹è±¡â€‹**ä»…ä»…æ˜¯ä»£ç é€»è¾‘çš„å®¹å™¨**â€‹ï¼ˆæˆ–è€…è¯´æ˜¯â€œè“å›¾â€ï¼‰ï¼Œå®ƒ**ä¸å­˜å‚¨**ä»»ä½•å‚æ•°æ•°æ®ã€‚
  - å‚æ•°ï¼ˆParamsï¼‰å¿…é¡»å•ç‹¬å­˜æ”¾åœ¨ä¸€ä¸ªå­—å…¸é‡Œã€‚
  - ä½ ä¸èƒ½ç›´æ¥è°ƒç”¨ `model(x)`â€‹ï¼Œä½ å¿…é¡»æ˜¾å¼åœ°æŠŠå‚æ•°ä¼ è¿›å»ï¼š`model.apply(params, x)`ã€‚

```python
from flax import linen as nn

class MyModule(nn.Module):
    # Some dataclass attributes, like hidden dimension, number of layers, etc. of the form:
    # varname : vartype
    hidden_dimï¼š int

    def setup(self):
        # Flax uses "lazy" initialization. This function is called once before you
        # call the model, or try to access attributes. In here, define your submodules etc.
        pass

    def __call__(self, x):
        # Function for performing the calculation of the module.
        pass
```

1. ç±»å±æ€§

    1. ä½ ä¸éœ€è¦å†™ `__init__`â€‹ æ¥æ¥æ”¶è¶…å‚æ•°ã€‚ä½ åªéœ€è¦åƒå†™å˜é‡ç±»å‹å£°æ˜ä¸€æ ·å†™åœ¨ç±»é‡Œï¼ŒFlax ä¼šè‡ªåŠ¨ä¸ºä½ ç”Ÿæˆä¸€ä¸ª `__init__` å‡½æ•°ã€‚
    2. å½“ä½ å®ä¾‹åŒ–æ—¶ï¼Œç›´æ¥å†™ `model = MyModule(hidden_dim=128)` å³å¯ã€‚
2. æƒ°æ€§åˆå§‹åŒ–

    1. åœ¨ PyTorch ä¸­ï¼Œ`__init__` æ˜¯åœ¨å¯¹è±¡åˆ›å»ºé‚£ä¸€åˆ»æ‰§è¡Œçš„ã€‚
    2. åœ¨ Flax ä¸­ï¼Œ`setup`â€‹ **ä¸æ˜¯**åœ¨ `model = MyModule(...)`â€‹ æ—¶æ‰§è¡Œçš„ã€‚å®ƒæ˜¯åœ¨ä½ ç¬¬ä¸€æ¬¡è°ƒç”¨ `model.init`â€‹ æˆ– `model.apply` çš„æ—¶å€™ï¼Œæ‰ä¼šè¢«â€œæƒ°æ€§â€è°ƒç”¨çš„ã€‚
    3. **ä¸ºä»€ä¹ˆï¼Ÿ**  å› ä¸º JAX éœ€è¦ç­‰å¾…è¾“å…¥æ•°æ®è¿›æ¥ï¼Œæ¨æ–­å‡ºæ•°æ®çš„ Shapeï¼Œæ‰èƒ½ç¡®å®šæƒé‡çš„å½¢çŠ¶ï¼ˆæ¯”å¦‚å…¨è¿æ¥å±‚çš„è¾“å…¥ç»´åº¦ï¼‰ï¼Œä»è€Œå®Œæˆåˆå§‹åŒ–ã€‚
3. â€‹`__call__(self, x)`â€‹ æ–¹æ³• â€”â€” å¯¹åº” PyTorch çš„ `forward`

â€

```python
class SimpleClassifier(nn.Module):
    num_hidden : int   # Number of hidden neurons
    num_outputs : int  # Number of output neurons

    def setup(self):
        # Create the modules we need to build the network
        # nn.Dense is a linear layer
        self.linear1 = nn.Dense(features=self.num_hidden)
        self.linear2 = nn.Dense(features=self.num_outputs)

    def __call__(self, x):
        # Perform the calculation of the model to determine the prediction
        x = self.linear1(x)
        x = nn.tanh(x)
        x = self.linear2(x)
        return x

class SimpleClassifierCompact(nn.Module):
    num_hidden : int   # Number of hidden neurons
    num_outputs : int  # Number of output neurons

    @nn.compact  # Tells Flax to look for defined submodules
    def __call__(self, x):
        # Perform the calculation of the model to determine the prediction
        # while defining necessary layers
        x = nn.Dense(features=self.num_hidden)(x)
        x = nn.tanh(x)
        x = nn.Dense(features=self.num_outputs)(x)
        return x

model = SimpleClassifier(num_hidden=8, num_outputs=1)\
print(model)
'''
SimpleClassifier(
    # attributes
    num_hidden = 8
    num_outputs = 1
)
'''

rng, inp_rng, init_rng = jax.random.split(rng, 3)
inp = jax.random.normal(inp_rng, (8, 2))  # Batch size 8, input size 2

# Initialize the model
params = model.init(init_rng, inp)
print(params)
'''
FrozenDict({
    params: {
        linear1: {
            kernel: DeviceArray([[ 0.31476864, -0.4647768 , -0.7862042 , -0.48842615,
                          -0.65373844,  0.3892545 ,  0.3038056 ,  0.04179859],
                         [-0.3298236 ,  1.1110363 ,  0.54909396, -0.8168818 ,
                           0.40057245, -0.8665987 ,  1.2087964 ,  1.0364622 ]],            dtype=float32),
            bias: DeviceArray([0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32),
        },
        linear2: {
            kernel: DeviceArray([[-0.27971813],
                         [-0.7466775 ],
                         [ 0.29791608],
                         [-0.26267236],
                         [-0.5084385 ],
                         [ 0.04573093],
                         [-0.47257012],
                         [ 0.50458497]], dtype=float32),
            bias: DeviceArray([0.], dtype=float32),
        },
    },
})
'''

model.apply(params, inp)
'''
DeviceArray([[-0.48368204],
             [ 0.04365474],
             [ 0.06668529],
             [-0.34203646],
             [ 0.4835147 ],
             [ 0.37424874],
             [ 0.14232653],
             [-0.5916512 ]], dtype=float32)
'''
```

â€

### The Data

ç›´æ¥ä½¿ç”¨pytorchä¾¿å¥½ï¼Œä½†æ˜¯æœ‰ä¸€äº›ç»†èŠ‚éœ€è¦æ³¨æ„ï¼š

â€‹**å”¯ä¸€çš„å¾®å°åŒºåˆ«**ï¼š

- åœ¨ `generate_continuous_xor`â€‹ å‡½æ•°é‡Œï¼Œä½œè€…ä½¿ç”¨äº† `numpy.random`â€‹ è€Œä¸æ˜¯ `torch.randn`â€‹ã€‚è¿™æ˜¯ä¸ºäº†ä¿è¯éšæœºæ•°ç”Ÿæˆçš„é€»è¾‘å’Œ JAX ä¹‹å‰çš„ç« èŠ‚ä¿æŒä¸€è‡´ï¼ˆæ˜¾å¼ç§å­æ§åˆ¶ï¼‰ï¼Œä¹Ÿä¸ºäº†é¿å…å¼•å…¥ä¸å¿…è¦çš„ `torch.Tensor` ä¾èµ–ã€‚æ•°æ®æ˜¯ä»¥ pure NumPy array çš„å½¢å¼å­˜å‚¨çš„ã€‚

**The Data Loader Class (å…³é”®çš„èƒ¶æ°´ä»£ç )**

è¿™é‡Œæœ‰ä¸€ä¸ªå…³é”®çš„æŠ€æœ¯ç»†èŠ‚ï¼šâ€‹**Collate Function**ã€‚

é»˜è®¤æƒ…å†µä¸‹ï¼ŒPyTorch çš„ DataLoader ä¼šæŠŠä¸€å †æ•°æ® stack èµ·æ¥ï¼Œç„¶å**è‡ªåŠ¨è½¬æ¢ä¸º** **â€‹`torch.Tensor`â€‹**â€‹ã€‚ ä½†åœ¨ JAX çš„è®­ç»ƒå¾ªç¯é‡Œï¼Œæˆ‘ä»¬éœ€è¦çš„æ˜¯ **NumPy Array**ï¼ˆæˆ–è€…æ˜¯ JAX Arrayï¼‰ã€‚

æ‰€ä»¥ï¼Œæˆ‘ä»¬éœ€è¦**è¦†ç›–**é»˜è®¤çš„è½¬æ¢é€»è¾‘`def numpy_collate`ã€‚

```python
import torch.utils.data as data

class XORDataset(data.Dataset):

    def __init__(self, size, seed, std=0.1):
        """
        Inputs:
            size - Number of data points we want to generate
            seed - The seed to use to create the PRNG state with which we want to generate the data points
            std - Standard deviation of the noise (see generate_continuous_xor function)
        """
        super().__init__()
        self.size = size
        self.np_rng = np.random.RandomState(seed=seed)
        self.std = std
        self.generate_continuous_xor()

    def generate_continuous_xor(self):
        # Each data point in the XOR dataset has two variables, x and y, that can be either 0 or 1
        # The label is their XOR combination, i.e. 1 if only x or only y is 1 while the other is 0.
        # If x=y, the label is 0.
        data = self.np_rng.randint(low=0, high=2, size=(self.size, 2)).astype(np.float32)
        label = (data.sum(axis=1) == 1).astype(np.int32)
        # To make it slightly more challenging, we add a bit of gaussian noise to the data points.
        data += self.np_rng.normal(loc=0.0, scale=self.std, size=data.shape)

        self.data = data
        self.label = label

    def __len__(self):
        # Number of data point we have. Alternatively self.data.shape[0], or self.label.shape[0]
        return self.size

    def __getitem__(self, idx):
        # Return the idx-th data point of the dataset
        # If we have multiple things to return (data point and label), we can return them as tuple
        data_point = self.data[idx]
        data_label = self.label[idx]
        return data_point, data_label

dataset = XORDataset(size=200, seed=42)
print("Size of dataset:", len(dataset))
print("Data point 0:", dataset[0])

# This collate function is taken from the JAX tutorial with PyTorch Data Loading
# https://jax.readthedocs.io/en/latest/notebooks/Neural_Network_and_Data_Loading.html
def numpy_collate(batch):
    if isinstance(batch[0], np.ndarray):
        return np.stack(batch)
    elif isinstance(batch[0], (tuple,list)):
        transposed = zip(*batch)
        return [numpy_collate(samples) for samples in transposed]
    else:
        return np.array(batch)

data_loader = data.DataLoader(dataset, batch_size=8, shuffle=True, collate_fn=numpy_collate)

# next(iter(...)) catches the first batch of the data loader
# If shuffle is True, this will return a different batch every time we run this cell
# For iterating over the whole dataset, we can simple use "for batch in data_loader: ..."
data_inputs, data_labels = next(iter(data_loader))

# The shape of the outputs are [batch_size, d_1,...,d_N] where d_1,...,d_N are the
# dimensions of the data point returned from the dataset class
print("Data inputs", data_inputs.shape, "\n", data_inputs)
print("Data labels", data_labels.shape, "\n", data_labels)
```

Data inputs (8, 2)  
 [[ 1.0504987   1.0865755 ]  
 [ 0.02809919 -0.06226995]  
 [ 0.06141667  1.0757508 ]  
 [ 0.08356921 -0.11297069]  
 [ 1.0324166  -0.01301431]  
 [ 1.0024511   0.04979983]  
 [ 0.3078881   0.11195749]  
 [ 1.0371146   0.9396015 ]]  
Data labels (8,)  
 [0 0 1 0 1 1 0 0]

### Optimization

ä¸¤ä¸ªæ–°æ¦‚å¿µï¼š**Optax**ï¼ˆä¼˜åŒ–å™¨åº“ï¼‰å’Œ **TrainState**ï¼ˆè®­ç»ƒçŠ¶æ€å®¹å™¨ï¼‰ã€‚

```python
import optax
# Input to the optimizer are optimizer settings like learning rate
optimizer = optax.sgd(learning_rate=0.1)
from flax.training import train_state
model_state = train_state.TrainState.create(apply_fn=model.apply,
                                            params=params,
                                            tx=optimizer)
```

- â€‹**PyTorch**â€‹: `optimizer = torch.optim.SGD(model.parameters(), lr=0.1)`â€‹ã€‚ä½ éœ€è¦æŠŠæ¨¡å‹çš„å‚æ•°**ä¼ ç»™**ä¼˜åŒ–å™¨ï¼Œä¼˜åŒ–å™¨å†…éƒ¨ä¼šç®¡ç†è¿™äº›å‚æ•°çš„æŒ‡é’ˆã€‚
- **JAX/Optax**: `optax.sgd`â€‹ ä»…ä»…å®šä¹‰äº†ä¸€å¥— **â€œæ¢¯åº¦å˜æ¢è§„åˆ™â€ (Gradient Transformation)ã€‚** å®ƒ**ä¸æŒæœ‰**ä»»ä½•å‚æ•°ã€‚å®ƒåªæ˜¯ä¸€ä¸ªçº¯ç²¹çš„å‡½æ•°é€»è¾‘ï¼šâ€œç»™æˆ‘ä¸€ä¸ªæ¢¯åº¦ï¼Œæˆ‘å‘Šè¯‰ä½ å‚æ•°è¯¥æ€ä¹ˆå˜â€ã€‚

â€‹`TrainState`â€‹ æœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ª â€‹**Snapshot (å¿«ç…§)** ã€‚å®ƒåŒ…å«äº†æŸä¸€æ—¶åˆ»è®­ç»ƒæ‰€éœ€çš„æ‰€æœ‰ä¿¡æ¯ã€‚

- åœ¨ PyTorch ä¸­ï¼Œè®­ç»ƒçŠ¶æ€åˆ†æ•£åœ¨ `model`â€‹ (weights) å’Œ `optimizer` (momentum buffers) é‡Œã€‚
- åœ¨ JAX ä¸­ï¼Œ`TrainState` å°±æ˜¯å”¯ä¸€çš„çœŸç†æ¥æºï¼ˆSingle Source of Truthï¼‰ã€‚è¾“å…¥æ—§çš„ Stateï¼Œè¾“å‡ºæ–°çš„ Stateï¼Œè¿™å°±æ˜¯è®­ç»ƒçš„æœ¬è´¨ã€‚

```python
def calculate_loss_acc(state, params, batch):
    data_input, labels = batch
    
    # 1. å‰å‘ä¼ æ’­
    # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬æ˜¾å¼ä¼ é€’ paramsã€‚
    # è™½ç„¶ state.params é‡Œä¹Ÿæœ‰å‚æ•°ï¼Œä½†ä¸ºäº†æ±‚å¯¼ï¼Œ
    # æˆ‘ä»¬å¿…é¡»æŠŠ params ä½œä¸ºä¸€ä¸ªç‹¬ç«‹çš„å‚æ•°æš´éœ²ç»™ jax.grad
    logits = state.apply_fn(params, data_input).squeeze(axis=-1)
    
    # 2. è®¡ç®— Loss (Binary Cross Entropy)
    # ä½¿ç”¨ logits è€Œä¸æ˜¯ sigmoid åçš„æ¦‚ç‡ï¼Œæ˜¯ä¸ºäº†æ•°å€¼ç¨³å®šæ€§ (Numerical Stability)
    loss = optax.sigmoid_binary_cross_entropy(logits, labels).mean()
    
    # 3. è®¡ç®— Accuracy (ä»…ç”¨äºç›‘æ§ï¼Œä¸å‚ä¸æ±‚å¯¼)
    acc = (logits > 0) == labels
    return loss, acc
```

ä¸Šé¢å®šä¹‰ä¸€ä¸ªæŸå¤±å‡½æ•°åœ¨Trainingæ—¶ä¼šè¢«ä½¿ç”¨ã€‚â€‹**â€‹`calculate_loss_acc`â€‹**: ä¸€ä¸ªè®¡ç®—å›¾å‡½æ•° $L(w, x)$ã€‚

### Training

è‘—åçš„ JAX è®­ç»ƒå¾ªç¯ï¼š

$$
State_{t+1} = \text{Update}(State_t, \nabla L(State_t.\text{params}, \text{batch}))
$$

â€‹`train_step`: çµé­‚å‡½æ•° (The Soul of JAX Training)

è¿™æ˜¯ JAX åŒºåˆ«äº PyTorch çš„æ ¸å¿ƒæ‰€åœ¨ã€‚ä½ éœ€è¦å†™ä¸€ä¸ª**æ— å‰¯ä½œç”¨çš„çº¯å‡½æ•°**æ¥æè¿°â€œä¸€æ­¥è®­ç»ƒâ€å‘ç”Ÿäº†ä»€ä¹ˆã€‚

```python
@jax.jit  # Jit the function for efficiency
def train_step(state, batch):
    # Gradient function
    grad_fn = jax.value_and_grad(calculate_loss_acc,  # Function to calculate the loss
                                 argnums=1,  # Parameters are second argument of the function
                                 has_aux=True  # Function has additional outputs, here accuracy
                                )
    # Determine gradients for current model, parameters and batch
    (loss, acc), grads = grad_fn(state, state.params, batch)
    # Perform parameter update with gradients and optimizer
    state = state.apply_gradients(grads=grads)
    # Return state and any other value we might want
    return state, loss, acc

def train_model(state, data_loader, num_epochs=100):
    for epoch in tqdm(range(num_epochs)):
        for batch in data_loader:
            # è¿™é‡Œçš„ batch å·²ç»æ˜¯ NumPy array äº†
            # state = æ–°çš„ state
            state, loss, acc = train_step(state, batch)
            # æ³¨æ„ï¼šè¿™é‡Œä¸éœ€è¦æ‰‹åŠ¨æŠŠæ•°æ®æ¬åˆ° GPUï¼ŒJAX ä¼šè‡ªåŠ¨å¤„ç†
    return state

trained_model_state = train_model(model_state, train_data_loader, num_epochs=100)

from flax.training import checkpoints
checkpoints.save_checkpoint(ckpt_dir='my_checkpoints/',  # Folder to save checkpoint in
                            target=trained_model_state,  # What to save. To only save parameters, use model_state.params
                            step=100,  # Training step or other metric to save best model on
                            prefix='my_model',  # Checkpoint file name prefix
                            overwrite=True   # Overwrite existing checkpoint files
                           )
loaded_model_state = checkpoints.restore_checkpoint(
                                             ckpt_dir='my_checkpoints/',   # Folder with the checkpoints
                                             target=model_state,   # (optional) matching object to rebuild state in
                                             prefix='my_model'  # Checkpoint file name prefix
                                            )
```

### Evaluation

```python
@jax.jit  # Jit the function for efficiency
def eval_step(state, batch):
    # Determine the accuracy
    _, acc = calculate_loss_acc(state, state.params, batch)
    return acc
test_dataset = XORDataset(size=500, seed=123)
# drop_last -> Don't drop the last batch although it is smaller than 128
test_data_loader = data.DataLoader(test_dataset,
                                   batch_size=128,
                                   shuffle=False,
                                   drop_last=False,
                                   collate_fn=numpy_collate)
def eval_model(state, data_loader):
    all_accs, batch_sizes = [], []
    for batch in data_loader:
        batch_acc = eval_step(state, batch)
        all_accs.append(batch_acc)
        batch_sizes.append(batch[0].shape[0])
    # Weighted average since some batches might be smaller
    acc = sum([a*b for a,b in zip(all_accs, batch_sizes)]) / sum(batch_sizes)
    print(f"Accuracy of the model: {100.0*acc:4.2f}%")
eval_model(trained_model_state, test_data_loader)
```

â€‹<kbd>å‚æ•°ç»‘å®š (Binding)</kbd>

è¯„ä¼°éƒ¨åˆ†é™¤äº†æ ‡å‡†çš„ `eval_step`â€‹ å¤–ï¼Œæ•™ç¨‹è¿˜ä»‹ç»äº†ä¸€ä¸ªå¾ˆå®ç”¨çš„åŠŸèƒ½ï¼šâ€‹**â€‹`model.bind`â€‹**ã€‚

åœ¨è®­ç»ƒæ—¶ï¼Œä¸ºäº†é…åˆ `jax.grad`â€‹ï¼Œæˆ‘ä»¬ä¸å¾—ä¸æŠŠå‚æ•° `params`â€‹ å•ç‹¬æ‹å‡ºæ¥ä¼ æ¥ä¼ å»ï¼š`model.apply(params, x)`ã€‚è¿™å†™èµ·æ¥å¾ˆç´¯ã€‚

**Binding (ç»‘å®š)**  è®©ä½ å¯ä»¥æš‚æ—¶å›å½’ç±»ä¼¼ PyTorch çš„ä½“éªŒï¼š

```python
# 1. å°†è®­ç»ƒå¥½çš„å‚æ•° "ç»‘" å›æ¨¡å‹ç»“æ„
trained_model = model.bind(trained_model_state.params)

# 2. ç°åœ¨ä½ å¯ä»¥åƒ PyTorch ä¸€æ ·ç›´æ¥è°ƒç”¨äº†ï¼
# ä¸éœ€è¦å†ä¼  params äº†
out = trained_model(data_input)
```

â€

## âœ¨ The Fancy Bits âœ¨

â€‹<kbd>JAX çš„ä¸‰å¤§ç¥æŠ€</kbd>

1. è‡ªåŠ¨å‘é‡åŒ–ï¼š`jax.vmap` (Auto-Vectorization)

è¿™æ˜¯ JAX æœ€â€œé­”æ³•â€çš„åŠŸèƒ½ã€‚

- â€‹**ç—›ç‚¹**â€‹ï¼šåœ¨ PyTorch ä¸­ï¼Œæˆ‘ä»¬å†™æ¨¡å‹æ—¶å¿…é¡»æ—¶åˆ»æƒ³ç€ Batch ç»´åº¦ã€‚æ¯”å¦‚å†™ä¸€ä¸ªçº¿æ€§å±‚ï¼Œå¿…é¡»å†™ `x @ w` è¿˜è¦æ³¨æ„ç»´åº¦åŒ¹é…ã€‚
- â€‹**JAX å“²å­¦**â€‹ï¼šâ€‹**åªå†™å¤„ç†å•ä¸ªæ ·æœ¬çš„é€»è¾‘**ã€‚

  - ä½ å®šä¹‰å‡½æ•° `f(x)`â€‹ å¤„ç†å½¢çŠ¶ä¸º `(C, H, W)` çš„å•å¼ å›¾ç‰‡ã€‚
  - è°ƒç”¨ `vmap(f)`â€‹ï¼Œå®ƒè‡ªåŠ¨æŠŠå‡½æ•°è½¬æ¢æˆèƒ½å¤„ç† `(B, C, H, W)` çš„ç‰ˆæœ¬ã€‚
- â€‹**æ ¸å¿ƒå‚æ•°** **â€‹`in_axes`â€‹**ï¼š

  -  `in_axes=(0, None, None)` æ˜¯ä»€ä¹ˆæ„æ€ï¼Ÿ
  - å‡è®¾å‡½æ•°æ˜¯ `simple_linear(x, w, b)`ã€‚
  - â€‹`0`â€‹: å¯¹ç¬¬ä¸€ä¸ªå‚æ•° `x`ï¼Œæ²¿ç€ç¬¬ 0 ç»´è¿›è¡Œ Batch åŒ–ï¼ˆæ¯å¼ å›¾ä¸ä¸€æ ·ï¼‰ã€‚
  - â€‹`None`â€‹: å¯¹ç¬¬äºŒã€ä¸‰ä¸ªå‚æ•° `w, b`â€‹ï¼Œ**ä¸**è¿›è¡Œ Batch åŒ–ï¼ˆæ‰€æœ‰å›¾å…±ç”¨åŒä¸€å¥—æƒé‡ï¼‰ã€‚
- â€‹**æ„ä¹‰**ï¼šè¿™æ¶ˆç­äº†å¤æ‚çš„ Batch ç»´åº¦æ‰‹åŠ¨å¯¹é½å·¥ä½œï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å¤æ‚çš„ Transformer Attention Mask æˆ–é«˜ç»´ç‰©ç†æ¨¡æ‹Ÿæ—¶æä¸ºæœ‰æ•ˆã€‚

2. å¹¶è¡Œè®¡ç®—ï¼š`jax.pmap` (Parallel Map)

- â€‹**åŒºåˆ«**ï¼š

  - â€‹`vmap`â€‹ æ˜¯åœ¨â€‹**å•ä¸ªè®¾å¤‡**ï¼ˆæˆ–é€»è¾‘è®¾å¤‡ï¼‰ä¸Šè¿›è¡Œå‘é‡åŒ–ï¼ˆVectorizationï¼‰ã€‚
  - â€‹`pmap`â€‹ æ˜¯åœ¨â€‹**å¤šä¸ªè®¾å¤‡**ï¼ˆå¤š GPU/TPUï¼‰ä¸Šè¿›è¡Œå¹¶è¡ŒåŒ–ï¼ˆParallelizationï¼‰ã€‚
- â€‹**SPMD èŒƒå¼**â€‹ï¼šè¿™æ˜¯ **Single Program Multiple Data** çš„ä½“ç°ã€‚ä½ å†™ä¸€ä»½ä»£ç ï¼Œ`pmap` æŠŠå®ƒåˆ†å‘åˆ° 8 å¼ å¡ä¸ŠåŒæ—¶è·‘ï¼Œæ¯å¼ å¡å¤„ç†ä¸åŒçš„æ•°æ®åˆ†ç‰‡ã€‚

3. PyTrees ä¸ `tree_map`

- â€‹**å®šä¹‰**â€‹ï¼šJAX æŠŠæ‰€æœ‰åµŒå¥—çš„å®¹å™¨ï¼ˆList of Dicts, Tuple of Arrays ç­‰ï¼‰ç»Ÿç§°ä¸º â€‹**PyTree**â€‹ã€‚Flax çš„ `params` å°±æ˜¯å…¸å‹çš„ PyTreeã€‚
- â€‹**å·¥å…·**ï¼š

  - â€‹`jax.tree_map(f, tree)`â€‹: å¯¹æ ‘é‡Œçš„æ¯ä¸€ä¸ªâ€œå¶å­èŠ‚ç‚¹â€ï¼ˆå¼ é‡ï¼‰åº”ç”¨å‡½æ•° `f`ã€‚
  - â€‹**åº”ç”¨åœºæ™¯**â€‹ï¼šæ¯”å¦‚ä½ è¦å¯¹æ‰€æœ‰å‚æ•°åº”ç”¨ L2 æ­£åˆ™åŒ–ï¼Œæˆ–è€…æ‰“å°æ‰€æœ‰å±‚çš„å‚æ•°å½¢çŠ¶ï¼ˆå¦‚æˆªå›¾æ‰€ç¤ºï¼‰ï¼Œä¸éœ€è¦å†™é€’å½’å¾ªç¯ï¼Œä¸€è¡Œ `tree_map` æå®šã€‚

## ğŸ”ª The Sharp Bits ğŸ”ª

â€‹<kbd>JAX çš„â€œå‘â€</kbd>

1. åŠ¨æ€å½¢çŠ¶ (Dynamic Shapes) â€”â€” NLP/Graph é¢†åŸŸçš„å™©æ¢¦

- â€‹**é—®é¢˜**â€‹ï¼šJIT ç¼–è¯‘æ˜¯**é’ˆå¯¹ç‰¹å®š Input Shape ç‰¹åŒ–**çš„ã€‚

  - å¦‚æœä½ è¾“å…¥ shape `(8, 10)`ï¼ŒJAX ç¼–è¯‘ä¸€æ¬¡ã€‚
  - ä¸‹ä¸€æ¬¡è¾“å…¥ shape `(8, 11)`â€‹ï¼ŒJAX â€‹**è§¦å‘é‡ç¼–è¯‘ (Re-compilation)** ã€‚
- â€‹**åæœ**ï¼šå¦‚æœä½ å¤„ç† NLP å˜é•¿å¥å­ï¼Œæ¯å¥è¯é•¿åº¦éƒ½ä¸ä¸€æ ·ï¼ŒJAX å°±ä¼šä¸åœåœ°ç¼–è¯‘ï¼Œå¯¼è‡´è®­ç»ƒææ…¢ã€‚
- â€‹**è§£å†³æ–¹æ¡ˆ**â€‹ï¼šâ€‹**Padding (å¡«å……)** ã€‚é€šå¸¸æŠŠå¥å­ Pad åˆ°å›ºå®šçš„ bucket é•¿åº¦ï¼ˆæ¯”å¦‚ 32, 64, 128ï¼‰ï¼Œä»¥å‡å°‘ç¼–è¯‘æ¬¡æ•°ã€‚

2. è°ƒè¯• (Debugging)

- â€‹**ç°è±¡**â€‹ï¼šåœ¨ JIT å‡½æ•°é‡Œå†™ `print(x.shape)`â€‹ï¼Œå®ƒåªä¼šåœ¨**ç¬¬ä¸€æ¬¡**è¿è¡Œï¼ˆTracing é˜¶æ®µï¼‰æ‰“å°å‡ºæ¥ã€‚çœŸæ­£è®­ç»ƒè·‘äº†å‡ ä¸‡æ¬¡ï¼Œæ§åˆ¶å°æ˜¯é™é»˜çš„ã€‚
- â€‹**åŸå› **â€‹ï¼š`print` æ˜¯ Python å‰¯ä½œç”¨ï¼Œè¢« JAX Tracer å¿½ç•¥äº†ã€‚
- â€‹**è§£å†³æ–¹æ¡ˆ**ï¼š

  - ä½¿ç”¨ `jax.debug.print`ï¼ˆæ•™ç¨‹æœªæï¼Œä½†è¿™æ˜¯æ ‡å‡†è§£æ³•ï¼‰ã€‚
  - æˆ–è€…åœ¨è°ƒè¯•æ—¶æš‚æ—¶å»æ‰ `@jax.jit`ã€‚

3. æœ‰çŠ¶æ€æ¨¡å—ï¼šDropout ä¸ BatchNorm (æœ€éš¾ç†è§£çš„éƒ¨åˆ†)

å› ä¸º Flax æ¨¡å‹æ˜¯æ— çŠ¶æ€çš„ï¼Œæ‰€ä»¥é‡åˆ°æœ¬èº«éœ€è¦â€œçŠ¶æ€â€çš„å±‚æ—¶ï¼Œå¤„ç†é€»è¾‘ä¼šå˜å¾—å¾ˆå¤æ‚ã€‚

- â€‹**Dropout**ï¼š

  - å®ƒåœ¨è®­ç»ƒæ—¶éœ€è¦éšæœºæ€§ã€‚
  - â€‹**JAX è§£æ³•**â€‹ï¼šä½ å¿…é¡»åœ¨ `apply` æ—¶æ˜¾å¼ä¼ å…¥ RNG Keyã€‚

```python
model.apply(params, x, rngs={'dropout': key})
```

å¹¶ä¸”éœ€è¦æ§åˆ¶å¼€å…³ï¼š`deterministic=False`â€‹ (Train) / `True` (Eval)ã€‚

â€‹**BatchNorm**ï¼š

- â€‹**ç—›ç‚¹**â€‹ï¼šBN å±‚åœ¨è®­ç»ƒæ—¶éœ€è¦æ›´æ–° `running_mean`â€‹ å’Œ `running_var`ã€‚è¿™ä¸æ˜¯é€šè¿‡æ¢¯åº¦æ›´æ–°çš„ï¼Œè€Œæ˜¯ç›´æ¥ç»Ÿè®¡æ›´æ–°çš„ã€‚
- â€‹**JAX è§£æ³•**â€‹ï¼šâ€‹**Mutable State**ã€‚
- è°ƒç”¨ `apply`â€‹ æ—¶ï¼Œä½ å¿…é¡»å‘Šè¯‰ Flaxï¼šâ€œè¯·æŠŠ `batch_stats` è¿™ä¸ªé›†åˆå½“ä½œå¯å˜çš„ï¼Œæ›´æ–°å®Œè¿˜ç»™æˆ‘â€ã€‚

```python
# è®­ç»ƒæ—¶ï¼š
logits, new_batch_stats = model.apply(
    {'params': params, 'batch_stats': old_stats}, # è¾“å…¥æ—§çŠ¶æ€
    x,
    mutable=['batch_stats'] # å£°æ˜æˆ‘ä»¬è¦æ›´æ–°è¿™ä¸ª
)
# å¿…é¡»æ‰‹åŠ¨ä¿å­˜ new_batch_stats ä¾›ä¸‹ä¸€æ¬¡è¿­ä»£ä½¿ç”¨
```

è¿™æ˜¯ Flax åŠé€€æ–°æ‰‹çš„æœ€å¤§é—¨æ§›ä¹‹ä¸€ï¼š**ä½ å¿…é¡»æ‰‹åŠ¨ç»´æŠ¤éæ¢¯åº¦å‚æ•°çš„çŠ¶æ€æµè½¬**ã€‚
